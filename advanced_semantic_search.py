import numpy as np
from typing import List, Tuple
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
import pickle
import os
import warnings
warnings.filterwarnings('ignore')

class AdvancedSemanticSearchEngine:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–º—ã—Å–ª –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞,
    –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    """
    
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        try:
            self.model = SentenceTransformer(model_name)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: pip install sentence-transformers")
            raise
        
        self.documents = []
        self.doc_embeddings = None
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english')).union(set(stopwords.words('russian')))
        
    def preprocess_text(self, text: str) -> str:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É,
        —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤.
        """
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = word_tokenize(text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens 
                  if token not in self.stop_words and token.isalpha()]
        
        return ' '.join(tokens)
    
    def add_documents(self, documents: List[str]):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å.
        """
        self.documents = documents
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.doc_embeddings = self.model.encode(documents)
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∏–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞, –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
        """
        if self.doc_embeddings is None or len(self.documents) == 0:
            raise ValueError("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ add_documents() —Å–Ω–∞—á–∞–ª–∞.")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        similarities = cosine_similarity(query_embedding, self.doc_embeddings).flatten()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        # –í–æ–∑–≤—Ä–∞—Ç –ø–∞—Ä (–∏–Ω–¥–µ–∫—Å, —Å—Ö–æ–¥—Å—Ç–≤–æ) –¥–ª—è —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = [(idx, similarities[idx]) for idx in top_indices if similarities[idx] > 0]
        
        return results
    
    def save_model(self, filepath: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª.
        """
        model_data = {
            'documents': self.documents,
            'doc_embeddings': self.doc_embeddings
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, filepath: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞.
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        self.documents = model_data['documents']
        self.doc_embeddings = model_data['doc_embeddings']

def demo_advanced_search():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """
    # –ü—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    documents = [
        "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∑–∞–Ω–∏–º–∞—é—â–∞—è—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω.",
        "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–∏—Å—Ç–µ–º–∞–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞—Ç—å—Å—è –∏ —É–ª—É—á—à–∞—Ç—å—Å—è.",
        "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö.",
        "Python - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
        "–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫.",
        "–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∞–π—Ç–æ–≤ –∏ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HTML, CSS –∏ JavaScript.",
        "–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.",
        "–ê–ª–≥–æ—Ä–∏—Ç–º—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ.",
        "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–∞—Ä–∞–¥–∏–≥–º—É –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –º–µ—Ö–∞–Ω–∏–∫–µ.",
        "–ë–ª–æ–∫—á–µ–π–Ω - —ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å."
    ]
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞
    search_engine = AdvancedSemanticSearchEngine()
    search_engine.add_documents(documents)
    
    print("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
    print("=" * 70)
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
    queries = [
        "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",      # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –æ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏
        "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python",          # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –æ Python
        "–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç–æ–≤",                 # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –æ –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
        "–•—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",                 # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –æ –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö
        "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",                      # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º –æ –≥–ª—É–±–æ–∫–æ–º –æ–±—É—á–µ–Ω–∏–∏
        "–ö–∞–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –ø–æ–Ω–∏–º–∞—é—Ç —Ä–µ—á—å?",       # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å NLP
        "–£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö",               # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–æ —Å –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
    ]
    
    for query in queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        print("-" * 50)
        results = search_engine.search(query, top_k=3)
        
        if results:
            for rank, (idx, score) in enumerate(results, 1):
                print(f"{rank}. –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}")
                print(f"   –î–æ–∫—É–º–µ–Ω—Ç: {search_engine.documents[idx][:100]}...")
                print()
        else:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
    
    print("\n" + "=" * 70)
    print("–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º –ø–æ–∏—Å–∫–æ–º:")
    print("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏ –¥–∞–∂–µ –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –Ω–∞–ø—Ä—è–º—É—é.")

def compare_approaches():
    """
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ vs —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    """
    print("\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤:")
    print("1. –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫: –∏—â–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º–∏ —Å–ª–æ–≤–∞–º–∏")
    print("2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞")
    print("\n–ü—Ä–∏–º–µ—Ä:")
    print("–ó–∞–ø—Ä–æ—Å: '–ö–∞–∫ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã –ø–æ–Ω–∏–º–∞—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Ä–µ—á—å?'")
    print("–¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –Ω–µ –Ω–∞–π—Ç–∏: '–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–º–æ–≥–∞–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫'")
    print("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ—Ç —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–æ–Ω–∏–º–∞–Ω–∏—é —Å–º—ã—Å–ª–∞!")

if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')

    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')

    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet')
    
    demo_advanced_search()
    compare_approaches()